# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yl4Z2txjk5CcCOozIfF2WGhQVyk0EHSk
"""

"""
SEN12-FLOOD cleaning pipeline:
- Loops tiles, opens VV/VH/mask with rasterio
- Verifies shapes/CRS alignment
- Converts SAR backscatter to dB, normalizes per band
- Skips empty/corrupted tiles
- Saves cleaned tiles + manifest/dropped/summary
"""

import argparse
import csv
import json
import re
from pathlib import Path

import numpy as np
import rasterio
from rasterio.errors import RasterioIOError


def log(msg: str):
    print(f"[sen12-clean] {msg}")

# Commented out IPython magic to ensure Python compatibility.
# %pip install rasterio

def read_tif(path: Path):
    with rasterio.open(path) as src:
        arr = src.read()       # (bands, H, W)
        profile = src.profile  # width, height, dtype, crs, transform
        crs = src.crs
        transform = src.transform
    return arr, profile, crs, transform


def write_tif(path: Path, arr: np.ndarray, profile_like: dict):
    prof = profile_like.copy()
    prof.update(dtype=rasterio.float32, count=arr.shape[0])
    path.parent.mkdir(parents=True, exist_ok=True)
    with rasterio.open(path, "w", **prof) as dst:
        dst.write(arr.astype(np.float32))


def to_db(x: np.ndarray) -> np.ndarray:
    # Convert linear backscatter to dB safely
    return 10.0 * np.log10(np.clip(x, 1e-6, None))


def zscore_per_band(x: np.ndarray) -> np.ndarray:
    x = x.astype(np.float32, copy=True)
    for b in range(x.shape[0]):
        mu = float(np.mean(x[b]))
        sd = float(np.std(x[b]) + 1e-6)
        x[b] = (x[b] - mu) / sd
    return x


def is_valid(arr: np.ndarray) -> bool:
    return np.isfinite(arr).all() and np.any(arr)


def same_geo(a_prof, a_crs, a_tx, b_prof, b_crs, b_tx) -> bool:
    return (a_prof["width"] == b_prof["width"] and
            a_prof["height"] == b_prof["height"] and
            a_crs == b_crs and
            a_tx == b_tx)

def find_candidates(raw_dir: Path):
    """
    Pair files into tiles with VV, VH, and mask.
    The matcher is permissive: looks for 'VV', 'VH', and 'mask' in filenames.
    Returns: list of dict(tile_id, vv, vh, mask)
    """
    tif_files = list(raw_dir.rglob("*.tif"))

    def tile_stem(name: str):
        base = name[:-4] if name.lower().endswith(".tif") else name
        parts = base.split("_")
        # Drop a trailing band token if present
        if len(parts) >= 2 and parts[-1].lower() in {"vv", "vh", "s1vv", "s1vh", "mask"}:
            return "_".join(parts[:-1])
        if len(parts) >= 2 and re.match(r"^(s1)?v[vh]$", parts[-1].lower()):
            return "_".join(parts[:-1])
        if len(parts) >= 2 and re.search(r"(vv|vh|mask)", parts[-1].lower()):
            return "_".join(parts[:-1])
        return base

    buckets = {}
    for p in tif_files:
        stem = tile_stem(p.name)
        buckets.setdefault(stem, []).append(p)

    candidates = []
    for stem, files in buckets.items():
        vv = vh = mask = None
        for f in files:
            n = f.name.lower()
            if re.search(r"(?:^|_)vv(?:_|\.tif$)|s1vv", n):
                vv = f
            elif re.search(r"(?:^|_)vh(?:_|\.tif$)|s1vh", n):
                vh = f
            elif "mask" in n:
                mask = f
        if vv and vh and mask:
            candidates.append({"tile_id": stem, "vv": vv, "vh": vh, "mask": mask})

    return candidates

def clean_one(rec: dict, out_dir: Path, min_mask_pixels: int = 1):
    """
    Clean a single record dict(tile_id, vv, vh, mask).
    Returns (kept: bool, info: dict|str).
    """
    tile = rec["tile_id"]
    try:
        vv, vv_prof, vv_crs, vv_tx = read_tif(rec["vv"])
        vh, vh_prof, vh_crs, vh_tx = read_tif(rec["vh"])
        m,  m_prof,  m_crs,  m_tx  = read_tif(rec["mask"])

        # Make mask (1, H, W)
        if m.ndim == 2:
            m = m[np.newaxis, ...]
        elif m.shape[0] != 1:
            m = m[:1, ...]

        # Geo/shape checks
        if not same_geo(vv_prof, vv_crs, vv_tx, vh_prof, vh_crs, vh_tx):
            return False, "VV/VH georef mismatch"
        if not same_geo(vv_prof, vv_crs, vv_tx, m_prof, m_crs, m_tx):
            return False, "VV/Mask georef mismatch"

        if not (is_valid(vv) and is_valid(vh)):
            return False, "Invalid SAR (NaN or all zeros)"

        if int(m.sum()) < int(min_mask_pixels):
            return False, f"Mask too small (<{min_mask_pixels} px)"

        # Transform SAR → dB → z-score
        vv_db = zscore_per_band(to_db(vv))
        vh_db = zscore_per_band(to_db(vh))

        # Save outputs
        base = out_dir / tile
        out_vv = base.with_name(base.name + "_S1VV_clean.tif")
        out_vh = base.with_name(base.name + "_S1VH_clean.tif")
        out_mk = base.with_name(base.name + "_mask.tif")

        write_tif(out_vv, vv_db, vv_prof)
        write_tif(out_vh, vh_db, vv_prof)  # keep identical geo
        write_tif(out_mk, m, vv_prof)

        return True, {
            "tile": tile,
            "vv": str(out_vv),
            "vh": str(out_vh),
            "mask": str(out_mk),
            "width": vv_prof["width"],
            "height": vv_prof["height"],
            "crs": str(vv_prof.get("crs")),
            "transform": str(vv_prof.get("transform")),
            "mask_pixels": int(m.sum()),
        }

    except RasterioIOError as e:
        return False, f"RasterIO error: {e}"
    except Exception as e:
        return False, f"Error: {e}"

!pip install kaggle
from google.colab import files
files.upload()
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
import kagglehub
path = kagglehub.dataset_download("rhythmroy/sen12flood-flood-detection-dataset")

from google.colab import files
uploaded = files.upload()

!mkdir -p /content/data/sen12flood
!unzip -q sen12flood-flood-detection-dataset.zip -d /content/data/sen12flood

!ls /content/data/sen12flood | head

def main():
    parser = argparse.ArgumentParser(description="Clean SEN12-FLOOD tiles (VV/VH + mask).")
    parser.add_argument("--raw_dir", type=Path, required=True, help="Root directory of raw SEN12-FLOOD .tif files")
    parser.add_argument("--out_dir", type=Path, required=True, help="Output directory for cleaned tiles")
    parser.add_argument("--min_mask_pixels", type=int, default=1, help="Drop tiles with fewer flooded pixels than this")
    parser.add_argument("--limit", type=int, default=0, help="If >0, only process this many tiles (for testing)")
    args = parser.parse_args()

    args.out_dir.mkdir(parents=True, exist_ok=True)

    log(f"Scanning {args.raw_dir} …")
    candidates = find_candidates(args.raw_dir)
    log(f"Found {len(candidates)} candidates with VV/VH/mask.")

    kept_rows, dropped_rows = [], []
    processed = 0

    for rec in candidates:
        if args.limit and processed >= args.limit:
            break
        kept, info = clean_one(rec, args.out_dir, min_mask_pixels=args.min_mask_pixels)
        if kept:
            kept_rows.append(info)
            processed += 1
            if processed % 10 == 0:
                log(f"Processed {processed} tiles…")
        else:
            dropped_rows.append({"tile": rec["tile_id"], "reason": info})

    # Write manifest/dropped/summary
    if kept_rows:
        manifest_path = args.out_dir / "manifest.csv"
        with manifest_path.open("w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=list(kept_rows[0].keys()))
            writer.writeheader()
            writer.writerows(kept_rows)
        log(f"Saved manifest: {manifest_path}")
    else:
        log("No tiles kept; manifest not written.")

    if dropped_rows:
        dropped_path = args.out_dir / "dropped.csv"
        with dropped_path.open("w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=["tile", "reason"])
            writer.writeheader()
            writer.writerows(dropped_rows)
        log(f"Saved dropped: {dropped_path}")

    summary = {
        "candidates": len(candidates),
        "kept": len(kept_rows),
        "dropped": len(dropped_rows),
        "min_mask_pixels": args.min_mask_pixels,
    }
    with (args.out_dir / "summary.json").open("w") as f:
        json.dump(summary, f, indent=2)
    log(f"Summary: {summary}")

# === Colab Runner ===
from pathlib import Path
import sys
from google.colab import files

# Define paths
RAW_DIR = Path("/content/data/sen12flood")      # where you unzipped the Kaggle ZIP
OUT_DIR = Path("/content/clean_sen12")          # where to save cleaned data
OUT_DIR.mkdir(parents=True, exist_ok=True)

# Pass args manually for Colab
argv_backup = sys.argv[:]
sys.argv = [
    "clean_sen12.py",
    "--raw_dir", str(RAW_DIR),
    "--out_dir", str(OUT_DIR),
    "--min_mask_pixels", "64",
    "--limit", "5"  # test on 5 tiles first; set 0 for full run later
]
main()
sys.argv = argv_backup

# Zip and download cleaned output
!zip -r -q /content/clean_sen12.zip /content/clean_sen12
files.download("/content/clean_sen12.zip")